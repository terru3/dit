{"cells":[{"cell_type":"markdown","source":["# True DiT w/ Patel et al. CFG Loss"],"metadata":{"id":"Rlc0xALky8TH"},"id":"Rlc0xALky8TH"},{"cell_type":"code","source":["N_CLASS = 10\n","LABEL = 0 ## when generating images of a single digit\n","\n","UNCOND_LABEL = N_CLASS ### unconditional token. Note this will yield n_class <- n_class + 1\n","## cannot use -1, which gives nn.Embedding error\n","# LABEL_DROPOUT = 0.2\n","\n","TRAIN_GUIDANCE = 1\n","SAMPLE_GUIDANCE = 3"],"metadata":{"id":"ADaVguyXsvpe"},"id":"ADaVguyXsvpe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ## Label dropout util fn to employ in train function, applied in-place\n","# def drop_label(labels, p):\n","#   \"\"\"\n","#   labels = batch of conditional labels\n","#   p = dropout prob\n","#   \"\"\"\n","#   mask = np.random.rand(*labels.shape) < p\n","#   labels[mask] = UNCOND_LABEL"],"metadata":{"id":"FADcHUptHh9m"},"id":"FADcHUptHh9m","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Transformer architecture\n","N_HEAD = 32\n","N_LAYER = 8\n","N_EMBD = 256\n","# N_KV_HEAD = 4\n","N_FF = N_EMBD * 4\n","\n","PATCH_SIZE = 4\n","\n","## Diffusion architecture\n","T_MAX = 400\n","BETA_MIN = 1e-4\n","BETA_MAX = 0.02\n","SCHEDULE = 'linear'\n","\n","## Data and training\n","DATASET = 'MNIST'\n","BATCH_SIZE = 64\n","LR = 1e-3\n","\n","IMG_CHANNELS = (1 if DATASET == 'MNIST' else 3)\n","IMG_SIZE = (28 if DATASET == 'MNIST' else 32)\n","\n","## Batch/step-level hyperparameters\n","EPOCHS = 100\n","SAVE_EVERY = int(EPOCHS*0.05) # save model every x epochs\n","GENERATE_EVERY = int(EPOCHS*0.05) # generate image from model every x epochs\n","\n","### ––––TEMP–––– ###\n","SAVE_EVERY = 1\n","GENERATE_EVERY = 1\n","### ––––TEMP–––– ###\n","\n","## Step-level hyperparameters\n","PRINT_EVERY = 100 # print loss every x steps\n","\n","## Model loading\n","CHECKPOINT = False\n","LOAD_EPOCH = None\n","\n","START_EPOCH = LOAD_EPOCH if LOAD_EPOCH is not None else 0\n","\n","# path = '/content/drive/MyDrive/DiT'\n","path = '/content/drive/Shareddrives/DSU Better Transformer/DIT'\n","\n","MODEL_NAME = f\"ada_ln_patel_dit_cfg_{N_LAYER}_LAYERs_{N_HEAD}_HEADs_{N_EMBD}_EMBD_DIM_{T_MAX}_TMAX_{DATASET}\"\n","print(\"Model Name:\", MODEL_NAME)\n","print(f'Model will be saved every {SAVE_EVERY} epochs, and will generate images every {GENERATE_EVERY} epochs')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mryXaUHWg_7U","executionInfo":{"status":"ok","timestamp":1709685481570,"user_tz":480,"elapsed":6,"user":{"displayName":"TERRY MING","userId":"03637908485946588412"}},"outputId":"5f0af1ae-8eae-4cd9-fe1e-e7434604fec5"},"id":"mryXaUHWg_7U","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Name: ada_ln_patel_dit_cfg_8_LAYERs_32_HEADs_256_EMBD_DIM_400_TMAX_MNIST\n","Model will be saved every 1 epochs, and will generate images every 1 epochs\n"]}]},{"cell_type":"markdown","id":"5db8fd7e","metadata":{"id":"5db8fd7e"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"id":"42352838","metadata":{"id":"42352838"},"outputs":[],"source":["%%capture\n","!pip install einops\n","# !pip install torchinfo"]},{"cell_type":"code","execution_count":null,"id":"4a68755d","metadata":{"id":"4a68755d"},"outputs":[],"source":["import json\n","import os\n","import random\n","import re\n","import time\n","\n","import matplotlib.animation as animation\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.utils as vutils\n","\n","from einops import pack, rearrange\n","from IPython.display import HTML\n","from PIL import Image\n","from torch.backends.cuda import sdp_kernel, SDPBackend\n","from torch.utils.data import DataLoader\n","# from torchinfo import summary\n","from torchvision import datasets, transforms\n","from tqdm import tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"VCjVXbWfz-Og","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a49c548-d83b-408b-cbd1-73200b16e961","executionInfo":{"status":"ok","timestamp":1709685528604,"user_tz":480,"elapsed":25315,"user":{"displayName":"TERRY MING","userId":"03637908485946588412"}}},"id":"VCjVXbWfz-Og","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"08547899","metadata":{"id":"08547899"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"id":"8b67c201","metadata":{"id":"8b67c201"},"outputs":[],"source":["def set_seed(seed = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # torch.cuda.manual_seed_all(seed) # if multi-GPU\n","    torch.backends.cudnn.deterministic=True # only applies to CUDA convolution operations\n","    torch.backends.cudnn.benchmark = False\n","    # usually CuDNN has heuristics as to which algorithm to pick. cudnn.benchmark benchmarks several algorithms and picks the fastest\n","    # often helpful if your input shapes are fixed and not changing a lot during training\n","    # however, this means it may pick a different algorithm even when the deterministic flag is set.\n","    # As such it is good practice to turn off cudnn.benchmark when turning on cudnn.deterministic\n","\n","set_seed()"]},{"cell_type":"code","execution_count":null,"id":"80d6225c","metadata":{"id":"80d6225c"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","source":["# # for flash attention\n","# backend_map = {\n","#     SDPBackend.MATH: {\"enable_math\": True, \"enable_flash\": False, \"enable_mem_efficient\": False},\n","#     SDPBackend.FLASH_ATTENTION: {\"enable_math\": False, \"enable_flash\": True, \"enable_mem_efficient\": False},\n","#     SDPBackend.EFFICIENT_ATTENTION: {\n","#         \"enable_math\": False, \"enable_flash\": False, \"enable_mem_efficient\": True}\n","# }"],"metadata":{"id":"EMnRD-r0nKOv"},"id":"EMnRD-r0nKOv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"FcHaZF0rOJ8I"},"id":"FcHaZF0rOJ8I"},{"cell_type":"code","source":["transform_fn = transforms.Compose([\n","    transforms.ToTensor(),  # Convert images to PyTorch tensors and [0, 1]\n","    transforms.Normalize((0.5,), (0.5,)) # [0,1] -> [-1, 1]\n","])\n","\n","# Download and load data\n","if DATASET == 'CIFAR-10':\n","    train_data = datasets.CIFAR10(\n","        root=\"./data\", train=True, download=True, transform=transform_fn\n","    )\n","    val_data = datasets.CIFAR10(\n","        root=\"./data\", train=False, download=True, transform=transform_fn\n","    )\n","elif DATASET == 'MNIST':\n","    train_data = datasets.MNIST(\n","        root=\"./data\", train=True, download=True, transform=transform_fn\n","    )\n","    val_data = datasets.MNIST(\n","        root=\"./data\", train=False, download=True, transform=transform_fn\n","    )\n","\n","# Create data loaders\n","train_loader = DataLoader(train_data,\n","                          shuffle=True,\n","                          batch_size=BATCH_SIZE)\n","val_loader = DataLoader(val_data,\n","                        shuffle=True,\n","                        batch_size=BATCH_SIZE)"],"metadata":{"id":"CMpW6mjmoY0i","executionInfo":{"status":"ok","timestamp":1709685530745,"user_tz":480,"elapsed":2145,"user":{"displayName":"TERRY MING","userId":"03637908485946588412"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea3d8d45-b850-4240-b0f1-ea544f263888"},"id":"CMpW6mjmoY0i","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 83688703.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 94785362.93it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 1648877/1648877 [00:00<00:00, 34360750.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 19301447.59it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# Diffusion"],"metadata":{"id":"S0YXkR09obkB"},"id":"S0YXkR09obkB"},{"cell_type":"code","source":["# extract relevant alpha/beta/etc. given timestep tensor t, reshape for compatibility with image x\n","def extract(a, t, x_shape):\n","    batch_size = t.shape[0]\n","    out = a.gather(-1, t.cpu())\n","    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n","# e.g. if x_shape is [batch_size, 3, 128, 128], then we get an output of [batch_size, 1, 1, 1]"],"metadata":{"id":"QzgSyji7x1LD"},"id":"QzgSyji7x1LD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DDPM(nn.Module):\n","    def __init__(self, t_max, beta_min, beta_max, schedule='linear', s=0.008):\n","\n","        assert schedule in ['linear', 'cosine'], \"beta variance schedule must be `linear` or `cosine`\"\n","        if schedule == 'cosine':\n","          assert s > 0, \"cosine offset must be positive\"\n","\n","        super().__init__()\n","        ## init variance schedule, alphas, etc.\n","        self.t_max = t_max\n","        self.beta_min, self.beta_max = beta_min, beta_max\n","\n","        ## linear schedule\n","        if schedule == 'linear':\n","            self.betas = torch.linspace(beta_min, beta_max, t_max)\n","        ## cosine schedule\n","        elif schedule == 'cosine':\n","            # https://arxiv.org/abs/2102.09672\n","            # β_t = 1 - ( a-bar_t / a-bar_{t-1}), where\n","            # a-bar_t = f(t) / f(0), where\n","            # f(t) = cos^2([π/2] * [((t/T) + s) /  (1+s)])\n","            steps = t_max + 1\n","            t = torch.linspace(0, t_max, steps)\n","            ft = torch.cos(((t / t_max) + s) / (1 + s) * torch.pi * 0.5) ** 2\n","            alphas_bar_t = ft / ft[0]\n","            betas = 1 - (alphas_bar_t[1:] / alphas_bar_t[:-1])\n","            self.betas = torch.clip(betas, beta_min, beta_max) ### changed from 0.9999 to beta_max\n","            ## observation: mixing cosine schedule w/ max 0.999 (rather than 0.02 max) is incompatible with current sampling code it seems\n","            # (at least with ε-prediction rather than x0), yields grey blurry images when sampling\n","\n","        ## to parameterize p and q\n","        self.alphas = 1 - self.betas\n","        self.alphas_bar = torch.cumprod(self.alphas, dim=0)\n","\n","    def forward(self, x0, t):\n","        ## i.e. q_sample\n","        noise = torch.randn_like(x0)\n","\n","        # recall xt  = √(a-bar_t)x0 + √(1-a-bar_t)ε\n","        # t will be a tensor when .forward is called, no need to create tensor version\n","        alpha_bar = extract(self.alphas_bar, t, x0.shape)\n","        mean = x0 * alpha_bar.sqrt()\n","        std = (1-alpha_bar).sqrt()\n","        xt = mean + std * noise\n","        return xt, noise\n","\n","    def backward(self, xt, model, labels, steps=None,\n","                 method='conditional', scale=SAMPLE_GUIDANCE, neg_label=UNCOND_LABEL):\n","\n","        assert method in ['conditional', 'cfg'], \"method must be `conditional` or `cfg`\"\n","        if method == 'cfg':\n","            assert isinstance(scale, (int, float, np.integer)) and scale >= 0, 'scale must be a float greater than or equal to 0'\n","\n","        if steps is None:\n","            steps = self.t_max\n","\n","        ## i.e. p_sample in a loop\n","        ## for i in reversed(range(0, timesteps)\n","        ## send through model, convert predicted noise (eta) into posterior mean, return mean + std*noise\n","        ## if last timestep, just return mean\n","\n","        ## fix variances simply as beta_t, like in paper, rather than learn\n","\n","        # note labels is a tensor already\n","\n","        B, C, H, W = xt.shape\n","\n","        ## list of how the images look after each denoising step\n","        xt_denoised_list = []\n","\n","        if method == 'cfg':\n","          neg_labels = torch.full((B,), neg_label, dtype=torch.long, device=device) ## unconditional label, or custom negative prompt\n","\n","        for t in reversed(range(0, steps)):\n","\n","            t_tensor = torch.full((B,), t, dtype=torch.long, device=device)\n","\n","            beta = extract(self.betas, t_tensor, xt.shape)\n","            alpha = extract(self.alphas, t_tensor, xt.shape)\n","            alpha_bar = extract(self.alphas_bar, t_tensor, xt.shape)\n","\n","            with torch.no_grad():\n","              noise_pred = model(xt, t_tensor, labels)\n","            if method == 'cfg':\n","              # if input label is already unconditional, skip\n","              if labels[0] == UNCOND_LABEL:\n","                pass\n","              else:\n","                with torch.no_grad():\n","                  neg_noise_pred = model(xt, t_tensor, neg_labels)\n","                noise_pred = scale*noise_pred + (1-scale) * neg_noise_pred\n","\n","\n","            # formula to convert predicted noise into model mean:\n","            # μθ(xt, t) = (1/√a_t) * (xt - (β_t / √(1-a-bar_t)) * noise_pred)\n","            mean = (1/alpha.sqrt()) * (xt - noise_pred * (beta / (1-alpha_bar).sqrt()))\n","            if t == 0:\n","              xt = mean # if last step, return noiseless image\n","            else:\n","              noise = torch.randn_like(xt)\n","              xt = mean + beta.sqrt() * noise\n","\n","            if t == steps - 1 or t % 20 == 0: ## first step / every 20 steps, save partially denoised image\n","            ##### TODO: change to depend on `steps`...\n","\n","              # normalize to [0,1] manually\n","              im_min, im_max = xt.min(), xt.max()\n","              image = (xt - im_min)/(im_max - im_min)\n","              xt_denoised_list.append(image)\n","\n","        return xt, xt_denoised_list"],"metadata":{"id":"7b6jGMA9obKL"},"id":"7b6jGMA9obKL","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"7b91dd4c","metadata":{"id":"7b91dd4c"},"source":["# Vision Transformer with AdaLN-Zero"]},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, n_embd, n_ff, dropout=0.1):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, n_ff),\n","            nn.GELU(),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(n_ff, n_embd),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"metadata":{"id":"qvzjOTJ2kgE9"},"id":"qvzjOTJ2kgE9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(\n","        self,\n","        n_embd,\n","        n_head,\n","        # n_kv_head,\n","        device,\n","        dropout=0.1,\n","    ):\n","        super().__init__()\n","\n","        self.n_embd = n_embd\n","        self.n_head = n_head\n","        self.head_dim = n_embd // n_head\n","        self.drop = nn.Dropout(p=dropout)\n","\n","        # self.n_kv_head = n_kv_head\n","        # self.n_repeat = self.n_head // self.n_kv_head\n","\n","        self.query = nn.Linear(n_embd, n_embd, bias=False)\n","        self.key = nn.Linear(n_embd, n_embd, bias=False)\n","        self.value = nn.Linear(n_embd, n_embd, bias=False)\n","\n","        # self.key = nn.Linear(n_embd, n_kv_head * self.head_dim, bias=False)\n","        # self.value = nn.Linear(n_embd, n_kv_head * self.head_dim, bias=False)\n","        self.out = nn.Linear(n_embd, n_embd, bias=False)\n","\n","        self.device = device\n","\n","    def split_heads(self, x, n_head):\n","        B, S, D = x.size()\n","        # split dimension into n_head * head_dim, then transpose the sequence length w/ n_head\n","        # output: [B, n_head, S, head_dim]\n","        return x.view(B, S, n_head, self.head_dim).transpose(1, 2)\n","\n","    def combine_heads(self, x):\n","        B, _, S, head_dim = x.size()  # _ is n_head which we will merge\n","        # output: [B, S, n_embd]\n","        return x.transpose(1, 2).contiguous().view(B, S, self.n_embd)\n","\n","    def scaled_dot_product(self, q, k, v, dropout):\n","        # q,k,v are [B, n_head, S, head_dim]\n","        # the key transpose sets up batch multiplication s.t. wei = [B, n_head, S, S]\n","        wei = q @ k.transpose(-2,-1) / np.sqrt(self.head_dim)\n","        # mask is [B, 1, S, S], so simply broadcasted across each head and works as expected\n","        wei = dropout(F.softmax(wei, dim=-1))\n","        out = wei @ v\n","        return out\n","\n","    def forward(self, x):\n","        # x: (B, S, n_embd)\n","        # Step 1 and 2: Project query, key, value, then split via reshaping\n","        q = self.split_heads(self.query(x), self.n_head)\n","        k = self.split_heads(self.key(x), self.n_head)\n","        v = self.split_heads(self.value(x), self.n_head)\n","        # k = self.split_heads(self.key(x), self.n_kv_head)\n","        # v = self.split_heads(self.value(x), self.n_kv_head)\n","\n","        # ## GQA\n","        # k, v = repeat_kv(k, v, self.n_repeat)\n","        # assert (\n","        #     k.shape[1] == self.n_head and v.shape[1] == self.n_head\n","        # ), \"key and value n_head do not match query n_head\"\n","        # # q, k, v [B, n_head, S, head_dim)\n","\n","        # Step 3: Compute scaled dot-product attention\n","        attn = self.scaled_dot_product(q, k, v, self.drop)\n","\n","        # # Step 3: Compute scaled dot-product attention\n","        # with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n","        #     try:\n","        #         attn = F.scaled_dot_product_attention(\n","        #             q,\n","        #             k,\n","        #             v,\n","        #             dropout_p=self.drop.p if self.device.type == \"cuda\" else 0\n","        #         ) # ViT: not causal ofc\n","        #     # CPU: Both fused kernels do not support non-zero dropout. (Dec 2023)\n","        #     except RuntimeError:\n","        #         print(\"FlashAttention is not supported. See warnings for reasons.\")\n","\n","        # Step 4 and 5: Concatenate attention scores, return projected output matrix\n","        out = self.out(self.combine_heads(attn))  # (B, S, n_embd)\n","        return out\n","\n","# # helper function for GQA\n","# def repeat_kv(k, v, n_repeat):\n","#     k = torch.repeat_interleave(k, repeats=n_repeat, dim=1)\n","#     v = torch.repeat_interleave(v, repeats=n_repeat, dim=1)\n","#     return k, v"],"metadata":{"id":"PaUt2JQlv3Xw"},"id":"PaUt2JQlv3Xw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def modulate(x, shift, scale):\n","  return x * (1+scale) + shift"],"metadata":{"id":"0hJcwf_ktguB"},"id":"0hJcwf_ktguB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Block(nn.Module):\n","    def __init__(\n","        self,\n","        n_embd,\n","        n_head,\n","        # n_kv_head,\n","        n_ff,\n","        device,\n","        norm_first,\n","        dropout\n","    ):\n","        super().__init__()\n","        self.sa = MultiHeadAttention(\n","            n_embd,\n","            n_head,\n","            # n_kv_head,\n","            device,\n","            dropout,\n","        )\n","        self.ff = MLP(n_embd, n_ff, dropout=dropout)\n","\n","        self.ln1 = nn.LayerNorm(n_embd, elementwise_affine=False) ######## no learnable parameters here!\n","        self.ln2 = nn.LayerNorm(n_embd, elementwise_affine=False)\n","        self.norm_first = norm_first\n","        self.drop = nn.Dropout(p=dropout)\n","\n","        self.adaLN = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(n_embd, 6*n_embd)\n","        )\n","\n","    def forward(self, x, c):\n","        ## c = (B, 1, n_embd)\n","        # residual connection (stream)\n","\n","        shift_sa, scale_sa, gate_sa, \\\n","        shift_mlp, scale_mlp, gate_mlp = self.adaLN(c).chunk(6, dim=-1)\n","\n","        # pre layer norm\n","        if self.norm_first:\n","            x = x + self.drop(gate_sa * self.sa(modulate(self.ln1(x), shift_sa, scale_sa)))\n","            x = x + self.drop(gate_mlp * self.ff(modulate(self.ln2(x), shift_mlp, scale_mlp)))\n","        else:\n","            x = self.ln1(x + self.drop(gate_sa * self.sa(modulate(x, shift_sa, scale_sa))))\n","            x = self.ln2(x + self.drop(gate_mlp * self.ff(modulate(x, shift_mlp, scale_mlp))))\n","        return x"],"metadata":{"id":"H61kJgPSwpNu"},"id":"H61kJgPSwpNu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PatchEmbedding(nn.Module):\n","    \"\"\"\n","    Applies patch embeddings to an image.\n","    \"\"\"\n","\n","    def __init__(self, patch_size, n_embd, in_channels=3):\n","        super().__init__()\n","\n","        self.patch_size = patch_size\n","        self.conv = nn.Conv2d(in_channels, n_embd, kernel_size=patch_size, stride=patch_size)\n","        # self.num_patches = (img_size // patch_size) ** 2\n","\n","    def forward(self, x):\n","        # (B, C, img_size, img_size) -> (B, num_patches, n_embd)\n","        x = self.conv(x) # (B, n_embd, img_size//patch_size, img_size//patch_size)\n","        x = rearrange(x, 'b c h w -> b (h w) c')\n","        # equivalent to above line: x = x.flatten(2).transpose(-1, -2)\n","        return x"],"metadata":{"id":"Oz4cA1S9oioV"},"id":"Oz4cA1S9oioV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DiT"],"metadata":{"id":"abpQff_poiCM"},"id":"abpQff_poiCM"},{"cell_type":"code","source":["class OutputLayer(nn.Module):\n","    \"\"\"\n","    The final layer of DiT. Projects (B, num_patches, n_embd) -> (B, num_patches, patch_size**2 * num_channels=C)\n","    \"\"\"\n","    def __init__(self, patch_size, n_embd, out_channels=3):\n","        super().__init__()\n","\n","        self.linear = nn.Linear(n_embd, patch_size * patch_size * out_channels)\n","        self.norm = nn.LayerNorm(n_embd)\n","        self.adaLN = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(n_embd, 2*n_embd)\n","        )\n","\n","    def forward(self, x, c):\n","        ## c = (B, 1, n_embd)\n","        shift, scale = self.adaLN(c).chunk(2, dim=-1)\n","        x = self.norm(x)\n","        x = modulate(x, shift, scale)\n","        x = self.linear(x)\n","        return x"],"metadata":{"id":"abYWqgJNGxCX"},"id":"abYWqgJNGxCX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DiT(nn.Module):\n","    \"\"\"\n","    Diffusion Transformer\n","\n","    img_size (int): Width/height of input images (assuming square)\n","    \"\"\"\n","\n","    def __init__(self,\n","                 n_embd,\n","                 n_head,\n","                 n_ff,\n","                 n_layer,\n","                 n_class,\n","                 n_channel,\n","                 img_size,\n","                 patch_size,\n","                 t_max,\n","                 beta_min,\n","                 beta_max,\n","                 schedule='linear',\n","                 s=0.008,\n","                 norm_first=True,\n","                 device=device,\n","                 dropout=0):\n","        super().__init__()\n","\n","        self.patch_embedding = PatchEmbedding(patch_size, n_embd, in_channels=n_channel)\n","\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size // patch_size) ** 2\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, n_embd)) ## original DiT paper uses 2D cos/sin embeddings\n","######### note no more 2+patches in AdaLN-Zero because we do not concatenate the conditioning info to x\n","\n","\n","        self.timestep_embedding = nn.Embedding(t_max, n_embd)\n","        self.label_embedding = nn.Embedding(n_class+1, n_embd) ### typically n_class. changed to n_class+1 to add CFG\n","\n","        self.blocks = nn.Sequential(\n","                                *[\n","                                    Block(\n","                                        n_embd,\n","                                        n_head,\n","                                        n_ff,\n","                                        device,\n","                                        norm_first,\n","                                        dropout\n","                                    )\n","                                    for i in range(n_layer)\n","                                ]\n","        )\n","\n","        self.mlp_head = OutputLayer(patch_size, n_embd, out_channels=n_channel)\n","\n","        self.drop = nn.Dropout(dropout)\n","        self.device = device\n","\n","        self.ddpm = DDPM(t_max, beta_min, beta_max, schedule, s)\n","        self.init_params()\n","\n","    def init_params(self, default_initialization=False):\n","        ## Xavier uniform——### unsure if appropriate for DiT\n","        # if not default_initialization:\n","        #     for name, p in self.named_parameters():\n","        #         if p.dim() > 1:\n","        #             # excludes layer norm and biases\n","        #             nn.init.xavier_uniform_(p)\n","        #         elif 'bias' in name:\n","        #             nn.init.zeros_(p)\n","\n","        ## key for Ada-LN Zero——initialize each block as the identity\n","        ## in particular, zero out the linear modulation layers and output layers\n","        for block in self.blocks:\n","            nn.init.constant_(block.adaLN[-1].weight, 0)\n","            nn.init.constant_(block.adaLN[-1].bias, 0)\n","        nn.init.constant_(self.mlp_head.adaLN[-1].weight, 0)\n","        nn.init.constant_(self.mlp_head.adaLN[-1].bias, 0)\n","        nn.init.constant_(self.mlp_head.linear.weight, 0)\n","        nn.init.constant_(self.mlp_head.linear.bias, 0)\n","\n","    def forward(self, x, t, label):\n","        ## x = (B, num_channels, img_size, img_size)\n","        ## t = (B, ) of timesteps\n","        ## label = (B, ) of labels\n","\n","        B = x.shape[0]\n","\n","        x = self.patch_embedding(x) # (B, num_patches, n_embd)\n","\n","        x += self.pos_embedding\n","        x = self.drop(x)\n","        # (B, num_patches, n_embd)\n","\n","        te = self.timestep_embedding(t).unsqueeze(1) # (B, 1, n_embd)\n","        lab = self.label_embedding(label).unsqueeze(1) # (B, 1, n_embd)\n","        c = te + lab ## (B, 1, n_embd)\n","\n","        # Note in AdaLN-Zero, rather than packing the t and label embeddings into x, we embed them separately and add together as 'c',\n","        # then send through every block and final layer\n","\n","        for block in self.blocks:\n","            x = block(x, c) # (B, num_patches, n_embd)\n","\n","        # last layer: project to (B, num_patches, patch_size**2 * num_channels=C)\n","        # unpatchify then returns img (B, C, H, W)\n","        x = self.mlp_head(x, c)\n","        img = rearrange(x, 'B (H W) (P1 P2 C) -> B C (H P1) (W P2)',\n","                      P1=self.patch_size,\n","                      P2=self.patch_size,\n","                      H = self.img_size // self.patch_size)\n","        return img"],"metadata":{"id":"ixMaVw4RIgmH"},"id":"ixMaVw4RIgmH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Patel et al. CFG Loss"],"metadata":{"id":"xXIKfYVQCwtg"},"id":"xXIKfYVQCwtg"},{"cell_type":"code","source":["def cfg_loss(preds, uncond_preds, noise):\n","  niket = (1+TRAIN_GUIDANCE) * preds - TRAIN_GUIDANCE * uncond_preds\n","  # niket = TRAIN_GUIDANCE * preds + (1-TRAIN_GUIDANCE) * uncond_preds ###### if using this line, train_guidance would need to be 2, not 1\n","  loss = F.mse_loss(niket, noise)\n","  return loss"],"metadata":{"id":"1wjwbmiXCyLG"},"id":"1wjwbmiXCyLG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialization"],"metadata":{"id":"VmIAyeTFmAex"},"id":"VmIAyeTFmAex"},{"cell_type":"code","source":["set_seed()\n","model = DiT(N_EMBD,\n","            N_HEAD,\n","            # N_KV_HEAD,\n","            N_FF,\n","            N_LAYER,\n","            n_class=N_CLASS,\n","            n_channel=IMG_CHANNELS,\n","            img_size=IMG_SIZE,\n","            patch_size=PATCH_SIZE,\n","            t_max=T_MAX,\n","            beta_min=BETA_MIN,\n","            beta_max=BETA_MAX,\n","            schedule=SCHEDULE,\n","            s=0.008,\n","            norm_first=True,\n","            device=device,\n","            dropout=0)\n","\n","model.to(device);\n","# summary(model)\n","\n","# criterion = nn.MSELoss()"],"metadata":{"id":"pVdn7-lkSTdP"},"id":"pVdn7-lkSTdP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(BATCH_SIZE, IMG_CHANNELS, IMG_SIZE, IMG_SIZE).to(device)\n","t = torch.randint(T_MAX, (BATCH_SIZE, )).to(device)\n","label = torch.full((BATCH_SIZE, ), LABEL).to(device)\n","\n","print(model(x, t, label).shape) # (B, IMG_CHANNELS, IMG_SIZE, IMG_SIZE)"],"metadata":{"id":"t424CZI9gBlx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6620ba5b-e518-4dbb-9405-850756e03e3e","executionInfo":{"status":"ok","timestamp":1709685558407,"user_tz":480,"elapsed":162,"user":{"displayName":"TERRY MING","userId":"03637908485946588412"}}},"id":"t424CZI9gBlx","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 1, 28, 28])\n"]}]},{"cell_type":"code","source":["print(f'Number of model parameters: {sum(p.numel() for p in model.parameters())}')"],"metadata":{"id":"pj6UZ7tlgIpA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eba36713-dfc0-410a-b9d6-6b95cdb515bd","executionInfo":{"status":"ok","timestamp":1709685558408,"user_tz":480,"elapsed":8,"user":{"displayName":"TERRY MING","userId":"03637908485946588412"}}},"id":"pj6UZ7tlgIpA","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of model parameters: 9718032\n"]}]},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"],"metadata":{"id":"CbCeteo6mBhf"},"id":"CbCeteo6mBhf","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ––––––––––––––––––––"],"metadata":{"id":"dQ3ZP6SJinlt"},"id":"dQ3ZP6SJinlt"},{"cell_type":"markdown","source":["# Loading"],"metadata":{"id":"Tbo5g6rwbpvq"},"id":"Tbo5g6rwbpvq"},{"cell_type":"code","source":["def load(model, optimizer):\n","    model.load_state_dict(torch.load(f'{path}/model/{MODEL_NAME}_epoch_{LOAD_EPOCH}.pt',\n","                                  map_location=device)[\"model_state_dict\"])\n","    print(\"Model loaded\")\n","\n","    optimizer.load_state_dict(torch.load(f'{path}/model/{MODEL_NAME}_epoch_{LOAD_EPOCH}.pt',\n","                                      map_location=device)[\"optimizer_state_dict\"])\n","    print(\"Optimizer loaded\")\n","\n","    with open(f'{path}/train_logs/{MODEL_NAME}_train_losses.json', 'r') as f3:\n","      train_losses = json.load(f3)\n","\n","    print(\"Train losses loaded\")\n","\n","    img_list = torch.load(f'{path}/train_logs/{MODEL_NAME}_img_list.pt')\n","\n","    print(\"Images loaded\")"],"metadata":{"id":"-ZMN8nZhbrBl"},"id":"-ZMN8nZhbrBl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"e5q_Bmyvyy8c"},"id":"e5q_Bmyvyy8c"},{"cell_type":"code","source":["def train(model,\n","          train_loader,\n","          criterion,\n","          optimizer,\n","          device,\n","          train_loss_list=None,\n","          img_list=None):\n","\n","    train_losses = train_loss_list if train_loss_list is not None else []\n","    img_list = img_list if img_list is not None else []\n","    train_times = []\n","\n","    model.train()\n","    model.to(device)\n","\n","    for epoch in range(START_EPOCH, EPOCHS):\n","        print(f\"Epoch: {epoch+1}\")\n","        for step, (img, label) in enumerate(train_loader):\n","\n","            img, label = img.to(device), label.to(device)\n","            start = time.perf_counter()\n","\n","            optimizer.zero_grad()\n","\n","            #### diffusion forward and backward here\n","            B = img.shape[0]\n","            t = torch.randint(1, model.ddpm.t_max, (B,), device=device).long()\n","            xt, noise = model.ddpm.forward(img, t)\n","            xt, noise = xt.to(device), noise.to(device)\n","\n","            # regular noise prediction\n","            preds = model(xt, t.to(device), label.to(device)).float()\n","\n","            # uncond noise prediction\n","            uncond_label = torch.full_like(label, UNCOND_LABEL, dtype=torch.long)\n","            uncond_preds = model(xt, t.to(device), uncond_label.to(device)).float()\n","\n","            # patel loss\n","            loss = cfg_loss(preds, uncond_preds, noise)\n","\n","            ## label dropout\n","            # drop_label(label, LABEL_DROPOUT)\n","\n","            # loss = criterion(preds, noise)\n","            loss.backward()\n","\n","            # Monitoring gradient norm\n","            grads = [\n","                    param.grad.detach().flatten()\n","                    for param in model.parameters()\n","                    if param.grad is not None\n","                ]\n","            norm = torch.cat(grads).norm()\n","\n","            optimizer.step()\n","\n","            train_times.append(time.perf_counter()-start)\n","\n","            if step % PRINT_EVERY == 0:\n","                print(f\"Step: {step} | Train Loss: {loss.item():.5f} |\",\n","                      f\"Grad Norm: {norm:.3f} | Train Batch Time: {np.mean(train_times):.3f}\")\n","\n","                train_losses.append(loss.item())\n","\n","        # save model and losses\n","        if epoch % SAVE_EVERY == 0:\n","            print(f\"Saving model, epoch {epoch+1}\")\n","            torch.save({\n","                \"model_state_dict\": model.state_dict(),\n","                \"optimizer_state_dict\": optimizer.state_dict()},\n","                f'{path}/model/{MODEL_NAME}_epoch_{epoch+1}.pt')\n","\n","            with open(f'{path}/train_logs/{MODEL_NAME}_train_losses.json', 'w') as f:\n","                json.dump(train_losses, f)\n","\n","            print(f\"Epoch {epoch+1}, model + data saved \\n\")\n","\n","        # generate and save images\n","        if (epoch % GENERATE_EVERY == 0) or (epoch == EPOCHS-1):\n","          print(f\"Generating images, epoch {epoch+1}\")\n","\n","          print(f\"Digit (CFG): {LABEL}\")\n","          labels = torch.full((BATCH_SIZE,), LABEL, dtype=torch.long, device=device)\n","          generate_single(model, labels=labels)\n","\n","          print(\"Every digit (CFG):\")\n","          img_list.append(generate_all_classes(model))\n","          generate_all_classes_steps(model)\n","\n","          # Patel's CFG loss does not allow for unconditional generation\n","          # print(\"Unconditional:\")\n","          # generate_uncond(model)\n","\n","          torch.save(img_list, f'{path}/train_logs/{MODEL_NAME}_img_list.pt')\n","          model.train()"],"metadata":{"id":"8F_Ytr15nblV"},"id":"8F_Ytr15nblV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_single(model, labels, scale=SAMPLE_GUIDANCE, neg_label=UNCOND_LABEL):\n","    \"\"\"\n","  Generates a batch of unconditional model output images\n","    \"\"\"\n","\n","    model.eval()\n","    noise = torch.randn(BATCH_SIZE, IMG_CHANNELS, IMG_SIZE, IMG_SIZE, device=device)\n","    diffused, _ = model.ddpm.backward(noise, model, labels,\n","                                      method='cfg', scale=scale, neg_label=neg_label)\n","\n","    imgs = vutils.make_grid(diffused, normalize=True)\n","    plt.imshow(imgs.permute(1,2,0).cpu())\n","    plt.show()\n","    return imgs"],"metadata":{"id":"JdZ5LBOsY-iK"},"id":"JdZ5LBOsY-iK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_all_classes(model, scale=SAMPLE_GUIDANCE, neg_label=UNCOND_LABEL):\n","    \"\"\"\n","    Generates rows of all classes.\n","    \"\"\"\n","\n","    num_rows = 4\n","\n","    labels = torch.arange(N_CLASS, device=device).repeat(num_rows)\n","    model.eval()\n","    noise = torch.randn(N_CLASS*num_rows, IMG_CHANNELS, IMG_SIZE, IMG_SIZE, device=device)\n","    diffused, _ = model.ddpm.backward(noise, model, labels,\n","                                      method='cfg', scale=scale, neg_label=neg_label)\n","    imgs = vutils.make_grid(diffused, normalize=True, nrow=N_CLASS) # confusingly, nrow = number in each row\n","    plt.imshow(imgs.permute(1,2,0).cpu())\n","    plt.show()\n","    return imgs"],"metadata":{"id":"80-qQ7pnMZ6G"},"id":"80-qQ7pnMZ6G","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_all_classes_steps(model, scale=SAMPLE_GUIDANCE, neg_label=UNCOND_LABEL):\n","    \"\"\"\n","    Generates rows of all classes.\n","    \"\"\"\n","\n","    labels = torch.arange(N_CLASS, device=device)\n","    model.eval()\n","    noise = torch.randn(N_CLASS, IMG_CHANNELS, IMG_SIZE, IMG_SIZE, device=device)\n","    diffused, history = model.ddpm.backward(noise, model, labels,\n","                                            method='cfg', scale=scale, neg_label=neg_label)\n","    history = torch.cat(history, dim=0)\n","\n","    imgs = vutils.make_grid(history, normalize=False, nrow=N_CLASS) # confusingly, nrow = number in each row\n","    plt.figure(figsize=(35,15))\n","    plt.imshow(imgs.permute(1,2,0).cpu())\n","    plt.show()\n","    return imgs"],"metadata":{"id":"ptuB-ZHBx_nI"},"id":"ptuB-ZHBx_nI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def generate_uncond(model):\n","#     \"\"\"\n","#     Generates images of random classes unconditionally.\n","#     NOTE: Does not use classifier-free guidance (method='cfg'), although it could with scale = 0\n","#     \"\"\"\n","\n","#     labels = torch.full((BATCH_SIZE,), UNCOND_LABEL, dtype=torch.long, device=device)\n","#     model.eval()\n","#     noise = torch.randn(BATCH_SIZE, IMG_CHANNELS, IMG_SIZE, IMG_SIZE, device=device)\n","#     diffused, _ = model.ddpm.backward(noise, model, labels)\n","#     imgs = vutils.make_grid(diffused, normalize=True)\n","#     plt.imshow(imgs.permute(1,2,0).cpu())\n","#     plt.show()\n","#     return imgs"],"metadata":{"id":"TAGvb60nhLgl"},"id":"TAGvb60nhLgl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Driver code"],"metadata":{"id":"tpSapoHvdol6"},"id":"tpSapoHvdol6"},{"cell_type":"code","source":["# TRAIN:\n","criterion = None\n","\n","if CHECKPOINT:\n","  load(model, optimizer)\n","  train(model, train_loader, criterion, optimizer, device, train_losses, img_list)\n","else:\n","  train(model, train_loader, criterion, optimizer, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"16NmFUshsgEo6gN0rufvZZVPfRegkEYOt"},"id":"RjLVLLOgJYpi","executionInfo":{"status":"error","timestamp":1709686426338,"user_tz":480,"elapsed":858432,"user":{"displayName":"TERRY MING","userId":"03637908485946588412"}},"outputId":"e93a2362-9819-42d9-b102-19cdc0c56a52"},"id":"RjLVLLOgJYpi","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Loss curve"],"metadata":{"id":"OScFb7jr6R71"},"id":"OScFb7jr6R71"},{"cell_type":"code","source":["with open(f'{path}/train_logs/{MODEL_NAME}_train_losses.json', 'r') as f:\n","    train_losses = json.load(f)\n","\n","plt.figure(figsize=(8,5))\n","plt.plot(train_losses[1:]) # skip very first loss\n","plt.xlabel(f\"Iterations (x {PRINT_EVERY})\")\n","plt.ylabel(\"Train Loss\")\n","plt.show()"],"metadata":{"id":"EZxyxOQLe6Q2"},"id":"EZxyxOQLe6Q2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization of diffusion"],"metadata":{"id":"IoG3klvsdote"},"id":"IoG3klvsdote"},{"cell_type":"code","source":["img_list = torch.load(f'{path}/train_logs/{MODEL_NAME}_img_list.pt')"],"metadata":{"id":"q052G_39vguc"},"id":"q052G_39vguc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Animation"],"metadata":{"id":"gfRrdRLej79L"},"id":"gfRrdRLej79L"},{"cell_type":"code","source":["fig = plt.figure(figsize=(8,8)) # assuming batch size 64\n","plt.axis(\"off\")\n","ims = [[plt.imshow(img.permute(1,2,0).cpu().numpy(), animated=True)] for img in img_list]\n","anim = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n","\n","HTML(anim.to_jshtml())"],"metadata":{"id":"sZUeHnsge7vd"},"id":"sZUeHnsge7vd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Side by side comparison"],"metadata":{"id":"BdssTm-nj8nP"},"id":"BdssTm-nj8nP"},{"cell_type":"code","source":["real_img, _ = next(iter(train_loader))\n","\n","# Plot the real images\n","plt.figure(figsize=(15,15))\n","plt.subplot(1,2,1)\n","plt.axis(\"off\")\n","plt.title(\"Real Images\")\n","plt.imshow(np.transpose(vutils.make_grid(real_img.to(device), padding=5, normalize=True).cpu(),(1,2,0)))\n","\n","# Plot the fake images from the last saved img_list\n","plt.subplot(1,2,2)\n","plt.axis(\"off\")\n","plt.title(\"Fake Images\")\n","plt.imshow(img_list[-1].permute(1,2,0).cpu().numpy())\n","plt.show()"],"metadata":{"id":"mctAOihhe83W"},"id":"mctAOihhe83W","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}